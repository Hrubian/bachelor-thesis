\chapter{Pandalyzer}

Our goal for this chapter is to present the implementation of the analysis tool based on the Abstract Interpretation
framework proposed in the previous chapter.
As the chapter name suggests, we named the tool Pandalyzer.

We start by stating the goal of the implementation - what features and functionalities should the tool have.
Then we present the architecture of the implemented solution from the software engineering point of view.
We talk about some design decisions done and discuss what are their consequences.
At last, we show how to build and run the tool and how it can be configured, and we also check whether solution satisfied
the goal stated at the beginning of the chapter.


\section{The Goal}

The implementation of an analyzer of a Pandas (Python) code based on Abstract Interpretation is a very
broad assignment specification, so we need to set some scope limitations for our implementation.
We do not want to support all Python language constructs and features as that would result in a full-blown Python
interpreter implementation which is definitely out of scope for this thesis.
On the other hand, our solution should be useful in practice, so the often used Python constructs should be supported.
This means that we definitely want to support assignments, function definitions and calling with return values and
arguments, unary and binary operators, if-statements, constants and variables of various types.
However, the implementation does not have to support classes, list comprehensions, lambdas, match statements, async code
or slices in a subscript operator, although the code should be extensible enough so that these constructs can be added
in the future development.


We also do not want to support all Pandas features as Pandas is a large project with very complex
(and sometimes inconsistent) API\@.
What we want to support are, again, the common features such as merging, grouping and aggregations, selection of
subset columns, renaming of columns and creating DataFrames and Series from lists or dictionaries.
We also want to support reading the DataFrames from a csv or other file formats in some way.
Less frequently used operations do not have to be supported, but the set of pandas operations should be easily
extensible with other operations.

The tool can be implemented as a command-line application and a user should be able to build and run it on Linux,
Windows and macOS\@.
It should accept a single python script filename as an argument and should print the analysis result to standard output
or a file.
The output format should be configurable.

Another requirement is the ability to continue in the analysis when a mistake in the code is found and the ability
to handle also mistakes that are not related to Pandas but pure Python. % todo weird

Pandas Dataframes are usually loaded from a csv files and also written to csv files.
The analyzer should not read or create any csv files.
It should accept an information from the user about existing csv files and the analysis result should contain
information about which csv files would be created by the script and what would be their structure.

\section{Architecture}

The programming language chosen for the implementation is Kotlin~\cite{kotlinDocs}.
The project uses Gradle~\cite{gradleDocs} as a build system, and it runs on JVM~\cite{jvmSpec}\@.
The source code can be found in the Pandalyzer git repository~\cite{pandalyzer}.

\subsection{The high-level idea}

Let us go over what the tool does.
It loads the Python script from the given input file.
Then it parses the code and creates and abstract syntax tree (AST) of the module.
Then it goes over the statements in the body of the module and interprets them while keeping the current context
containing all currently active variables, raised errors and warnings, etc.
Finally, it writes the result of the analysis to the standard output or to an output file (if provided).

The text above is a good short description of the tool, but it probably leaves the reader with many questions unresolved,
such as:
\begin{itemize}
    \item How is the tool run?
    \item How is the parsing done?
    \item How is the AST represented in the program?
    \item How are the currently active variables and other data represented in the context?
    \item How are the Pandas operations and other function checking implemented?
    \item What does the output of the tool look like?
    \item How does the tool accept the additional information about the input csv files?
    \item How does the tool handle non-deterministic or unknown data?
\end{itemize}

Our aim now is to answer all these (and other) questions and explain the implementation in more detail.

\subsection{Design Decision - Parsing and AST}

The tool needs to construct and abstract syntax tree of a Python module.
Implementing a Python parser from scratch would be a lot of unnecessary work and a potential source of bugs, so we
decided to use already existing solutions.
There is a Python module called \textbf{ast}~\cite{python-ast} capable of parsing a python module and creating an
abstract syntax tree.
However, since it is a Python module, we are not able to use it in Kotlin directly.
We choose an alternative approach.
We create a Python script that accepts a Python module, creates an abstract syntax tree using the \textbf{ast} module
and serializes the tree to a json format.
Then we use Kotlin Serialization~\cite{kotlin-serialization-guide} library to deserialize the json to the kotlin
representation.
The Python script can be found in the Pandalyzer repo~\cite{pandalyzer} in the path
/src/main/resources/python\_converter.py and uses serialization ideas from the ast2json Python module~\cite{ast2json}.
To avoid the need to run the converter script before running the Pandalyzer, the Pandalyzer calls the script instead of
the user, effectively hiding the information about the existence of the script from the user.
However, the fact that we use Python script implies that the python needs to be installed on the machine.

\begin{figure}[H]
    \caption{AST representation hierarchy}
    \label{fig:python_hierarchy}
    \centering
    \includegraphics[scale=0.5]{img/python_hierarchy}
\end{figure}


The AST representation in Kotlin uses hierarchy of classes and interfaces.
The figure~\ref{fig:python_hierarchy} shows a subset (the whole hierarchy is much larger) of the classes (yellow boxes),
and interfaces (blue boxes).

Each class represents a node in the abstract syntax tree.
The nodes of the tree have children depending on their semantics.
For example the node Module has a list of nodes Statements representing the body of the module.
Another good example is an IfStatement, which has test Expression as a child, list of statements as the body of the
if-statement and another list of statements as the body of the else branch.

Additionally, each node implementing the Statement interface has also information about the location of the statement
in the script file - line numbers and column numbers.

\subsection{Analysis Context}

When traversing the AST and interpreting the program, we have to keep track of all currently known variables.
Also, want to store the errors and warnings generated.
We introduce the Analysis Context structure for this purpose.
In the implementation, AnalysisContext is an interface providing us with function such as getStruct, upsertStruct,
addWarning, addError and more.
There are two classes implementing this interface - GlobalAnalysisContext and FunctionAnalysisContext.

GlobalAnalysisContext is the context that keeps track of all global variables and functions and python builtin functions
as well.
It also contains metadata about the csv files that we read from and write to.

FunctionAnalysisContext on the other hand is created when a function is invoked.
It contains a reference to outer analysis context and a global analysis context.
It redirects the addWarning and addError function to the outer analysis context, and it also redirects the getStruct
function to the global analysis context in case that it does not know the requested variable.
This ensures that the concept of global and local variables works properly.

When the analysis is done, the global analysis context has a summarize function, which returns a summary of the analysis.


\xxx{TODO}

\subsection{Python structures representation}

When we discussed the Analysis Context, we said that it keeps all currently active variables.
But how does a statically typed language like Kotlin keeps track of variables that have type resolved dynamically in
runtime?

The answer is polymorphism.
We define the interface PythonDataStructure.
Every Python data type has to implement the PythonDataStructure interface.
The interface contains methods representing what we can do with any Python data type.
There are functions for the implementation of binary operators such as add, subtract, multiply, etc., unary operators
such as unary plus or unary minus, subscript function (the square bracket operator), the invoke function
(the parenthesis operator) and the attribute function (used for the dot notation).
%There is also a boolValue function

All these functions return the following type:
\begin{verbatim}
   OperationResult<PythonDataStructure>
\end{verbatim}
So the operations return a PythonDataStructure, but it is wrapped inside the OperationResult.
The OperationResult is a sealed interface (meaning that all who implement the interface must be known at compile time).
There are three classes implementing the OperationResult interface: OK, Warning and Error.
OK signalizes that the operation succeeded, and it contains the result PythonDataStructure.
Warning tells us that the operation was successful but some warnings were raised during the execution.
The Warning type contains the result PythonDataStructure and a list of warnings - strings.
Last type is the Error.
It tells us that the operation failed, and it also provides a reason string.

We said that every Python data type has to implement the PythonDataStructure interface.
But what types do we define?
There are standard Python types such as PythonString, PythonInt, PythonBool, PythonNone, PythonList or PythonDict.
There are also functions (yes, function is also a PythonDataStructure).
This includes (subset of) Python builtin
functions such as \verb|print, list, len, int, or abs| as well as other functions defined in the current scope.
Another things that also implement the PythonDataStructure are imports.
Currently, the only supported import is PandasImport, but the implementation can be easily extended by others.
The Pandas data types support is added by creating DataFrame and Series classes that also implement the PythonDataStructure
interface.


\subsection{Nondeterminism, unknown values and error recovery}



\xxx{TODO null values, NondeterministicStructures, UnresolvedStructures, fork-join pattern on analysis context}


\subsection{Design Decision - Configuration file}

When we defined the goals of the implementation, we mentioned the fact that Pandas Dataframes are usually loaded
from a csv file.
We stated a requirement that the analyzer should not try to read the file.
The analyzer should instead receive the information about the csv file structures from the user in some other way.
This has the advantage that the csv files do not have to exist at all yet during the analysis as long as we know their
structure.

This, however, raises a question: How should the Pandalyzer receive the information from the user?
The following options were considered:
\begin{itemize}
    \item The user specifies the csv structures as a command-line arguments

    This was not accepted since it is not practical for the case when the set of csv files is bigger or the csv files
    have many columns.

    \item The user adds the information straight to the Python code via some annotation or nop operation

    This was also rejected since the user has to make changes to the code, and it could make the code less readable.

    \item The tool will ask the user for the information once it recognizes the read\_csv function

    The problem with this approach is that it needs a lot of interaction with the user, and it complicates automatic
    execution of the analysis.

    \item The user will provide a configuration file with the csv structures

    This approach has the advantage that it is relatively easy to specify many files that have many columns.
    Also, the configuration can be written just once in a whole project and be a part of the repository.
    So we chose this approach in the implementation.
\end{itemize}

The format of the configuration file can be best explained by an example.

\begin{lstlisting}[caption=An example configuration file, label={lst:config_example}, captionpos=b]
[file.csv]
column1 = "string"
column2 = "int"

[file2.csv]
columnA = "int"
columnB = "int"
columnC = "int"
\end{lstlisting}

As seen in the Listing~\ref{lst:config_example}, the format is inspired by a well known configuration format
TOML~\cite{toml_spec}.
We chose TOML because of its excellent readability.
However, we do not use the TOML language but only a small modified subset.
Definition of each file structure begins with a line containing the (relative or absolute) path to the file in the file
system surrounded by square brackets.
Note that this is already not a TOML, since the path to file can also contain dots or slashes which is not permitted in
TOML specification.
Following lines contain the specification of columns starting with column name, then equal sign and then the type of the
column surrounded by double quotes.

Other feature of the configuration is the regex-based file definition.
When we prepend the line containing definition of new file structure with the letter \verb|r|, the filename is
considered a regex pattern and each file matching the pattern will have the defined structure.
The idea is shown in the Listing~\ref{lst:regex_example}.

\begin{lstlisting}[caption=An example regex-based file definition, label={lst:regex_example}, captionpos=b]
r[^\d\d_file\.csv]
col = "string"
\end{lstlisting}

This means that files with names such as 01\_file.csv, 02\_file.csv, 99\_file.csv have one column of type string.

\section{User documentation}

\subsection{Building from source}

To build the Pandalyzer from sources, follow the steps below:
\begin{enumerate}
    \item Ensure that you have JDK (version \xxx{todo version}), Gradle (version \xxx{todo version}), Git
    and Python 3.x installed.
    \item Clone the Pandalyzer repository:

    \verb|git clone https://github.com/Hrubian/Pandalyzer.git|
    \item Navigate to the root folder of the repository:

    \verb|cd Pandalyzer|
    \item Run the Gradle bootstrap script:

    \verb|./gradlew build| or \verb|./gradlew.bat build| on Windows
\end{enumerate}

\subsection{Running the tool}

The build generates a .jar file \xxx{TODO where}.
This file can be run with the following command:
\verb|java --jar \xxx{todo file path}|.
The program accepts the following command-line arguments:
\begin{itemize}
    \item -h, --help - Prints usage information and exits
    \item -i, --input - The input python script to analyze \textbf{(mandatory)}
    \item -o, --output - The output file to store the analysis result to (standard output by default)
    \item -c, --config - The configuration file to read the file structures from (config.toml by default)
    \item -f, --format - The format of the analysis output, possible options: hr (human-readable), json (hr by default), csv
\end{itemize}

The behaviour and output of the program will be discussed in the next chapter.

\section{Future extensions}

\section*{Summary}
\addcontentsline{toc}{section}{Summary}